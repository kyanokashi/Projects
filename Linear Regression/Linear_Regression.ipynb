{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # used for scientific computing\n",
    "import pandas as pd # used for data analysis and manipulation\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will use a dataset containing housing prices in King County, USA. The dataset contains 5,000 observations with 18 features and a single target value - the house price.\n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv') # Make sure this cell runs regardless of your absolute path.\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7cd243e8b5fe5aa",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df['sqft_living'].values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Use [mean normalization](https://en.wikipedia.org/wiki/Feature_scaling) for the fearures (`X`) and the true labels (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on the features and true labels.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs (n features over m instances).\n",
    "    - y: True labels.\n",
    "\n",
    "    Returns a two vales:\n",
    "    - X: The mean normalized inputs.\n",
    "    - y: The mean normalized labels.\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.subtract(X, X.mean(axis=0))/ X.std(axis=0)\n",
    "    y = np.subtract(y, np.mean(y))/ np.std(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9bb6a28b6b6932fa",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into two datasets: \n",
    "1. The training dataset will contain 80% of the data and will always be used for model training.\n",
    "2. The validation dataset will contain the remaining 20% of the data and will be used for model evaluation. For example, we will pick the best alpha and the best features using the validation dataset, while still training the model using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train], X[idx_val]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'ro', ms=1, mec='k') # the parameters control the size, shape and color of the scatter plot\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Bias Trick\n",
    "\n",
    "Make sure that `X` takes into consideration the bias $\\theta_0$ in the linear model. Hint, recall that the predications of our linear model are of the form:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-44853962dc1651df",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "b1 = np.ones(np.shape(X_train)[0])\n",
    "X_train = np.column_stack((b1, X_train))\n",
    "b1 = np.ones(np.shape(X_val)[0])\n",
    "X_val = np.column_stack((b1, X_val))\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7d7fd68c1b24943",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Single Variable Linear Regression\n",
    "Simple linear regression is a linear regression model with a single explanatory varaible and a single target value. \n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "## Gradient Descent \n",
    "\n",
    "Our task is to find the best possible linear line that explains all the points in our dataset. We start by guessing initial values for the linear regression parameters $\\theta$ and updating the values using gradient descent. \n",
    "\n",
    "The objective of linear regression is to minimize the cost function $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where the hypothesis (model) $h_\\theta(x)$ is given by a **linear** model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$$\n",
    "\n",
    "$\\theta_j$ are parameters of your model. and by changing those values accordingly you will be able to lower the cost function $J(\\theta)$. One way to accopmlish this is to use gradient descent:\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$$\n",
    "\n",
    "In linear regresion, we know that with each step of gradient descent, the parameters $\\theta_j$ get closer to the optimal values that will achieve the lowest cost $J(\\theta)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the average squared difference between an obserbation's actual and\n",
    "    predicted values for linear regression.  \n",
    "\n",
    "    Input:\n",
    "    - X: inputs  (n features over m instances).\n",
    "    - y: true labels (1 value over m instances).\n",
    "    - theta: the parameters (weights) of the model being learned.\n",
    "\n",
    "    Returns a single value:\n",
    "    - J: the cost associated with the current set of parameters (single number).\n",
    "    \"\"\"\n",
    "\n",
    "    m = len(X)\n",
    "    h_theta = np.dot(X, theta)\n",
    "    difference = np.square(np.subtract(h_theta, y))\n",
    "    J = 1 / (2 * m) * np.sum(difference)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c1cfec24e144479",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta = np.array([-1, 2])\n",
    "J = compute_cost(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of the model using gradient descent using \n",
    "    the *training set*. Gradient descent is an optimization algorithm \n",
    "    used to minimize some (loss) function by iteratively moving in \n",
    "    the direction of steepest descent as defined by the negative of \n",
    "    the gradient. We use gradient descent to update the parameters\n",
    "    (weights) of our model.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "\n",
    "    m = len(X)\n",
    "    for i in range(num_iters):\n",
    "        J_history.append(compute_cost(X, y, theta))\n",
    "        h_theta = np.dot(X, theta)\n",
    "        difference = np.subtract(h_theta, y)\n",
    "        product = np.dot(X.T, difference) * alpha * (1 / m)\n",
    "        theta = np.subtract(theta, product)\n",
    "\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59b95cbea13e7fc1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 40000\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can evaluate the learning process by monitoring the loss as training progress. In the following graph, we visualize the loss as a function of the iterations. This is possible since we are saving the loss value at every iteration in the `J_history` array. This visualization might help us find problems with the code. Notice that since the network converges quickly, we are using logarithmic scale for the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def pinv(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the optimal values of the parameters using the pseudoinverse\n",
    "    approach as you saw in class using the *training set*.\n",
    "    \n",
    "    AX=B\n",
    "    \n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The optimal parameters of your model.\n",
    "\n",
    "    ########## DO NOT USE np.linalg.pinv ##############\n",
    "    \"\"\"\n",
    "\n",
    "    X_0 = np.dot(X.T,X)\n",
    "    X_1 = np.linalg.inv(X_0)\n",
    "    X_dager = np.dot(X_1,X.T)\n",
    "    pinv_theta = np.dot(X_dager,y)\n",
    "\n",
    "    return pinv_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the loss value for the theta calculated using the psuedo-inverse to our graph. This is another sanity check as the loss of our model should converge to the psuedo-inverse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5043aa5363cbe5c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can use a better approach for the implementation of `gradient_descent`. Instead of performing 40,000 iterations, we wish to stop when the improvement of the loss value is smaller than `1e-8` from one iteration to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def efficient_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of your model using the *training set*, but stop \n",
    "    the learning process once the improvement of the loss value is smaller \n",
    "    than 1e-8. This function is very similar to the gradient descent \n",
    "    function you already implemented.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "\n",
    "    m = len(X)\n",
    "    current = compute_cost(X, y, theta)\n",
    "    prev = float('inf')\n",
    "    while ((abs(current - prev) > 1e-8) and (num_iters > 0)):\n",
    "        prev = current\n",
    "        J_history.append(current)\n",
    "        h_theta = np.dot(X, theta)\n",
    "        difference = np.subtract(h_theta, y)\n",
    "        product = np.dot(X.T, difference) * alpha * (1 / m)\n",
    "        theta = np.subtract(theta, product)\n",
    "        current = compute_cost(X, y, theta)\n",
    "        num_iters -= 1\n",
    "\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6e2524d07523d950",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The learning rate is another factor that determines the performance of our model in terms of speed and accuracy. Complete the function `find_best_alpha`. Make sure you use the training dataset to learn the parameters (thetas) and use those parameters with the validation dataset to compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def find_best_alpha(X_train, y_train, X_val, y_val, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over provided values of alpha and train a model using the \n",
    "    *training* dataset. maintain a python dictionary with alpha as the \n",
    "    key and the loss on the *validation* set as the value.\n",
    "\n",
    "    Input:\n",
    "    - X_train, y_train, X_val, y_val: the training and validation data\n",
    "    - iterations: maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - alpha_dict: A python dictionary - {key (alpha) : value (validation loss)}\n",
    "    \"\"\"\n",
    "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 2, 3]\n",
    "    alpha_dict = {}\n",
    "\n",
    "    initial_theta = np.array([1, 1])\n",
    "\n",
    "    for alpha in alphas:\n",
    "        theta, J_history = efficient_gradient_descent(X_train, y_train, initial_theta, alpha, iterations)\n",
    "        cost = compute_cost(X_val, y_val, theta)\n",
    "        alpha_dict[alpha] = cost\n",
    "\n",
    "    return alpha_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b088fe7a10910a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyanokashi/opt/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "<ipython-input-46-08b878664b32>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  while ((abs(current - prev) > 1e-8) and (num_iters > 0)):\n"
     ]
    }
   ],
   "source": [
    "alpha_dict = find_best_alpha(X_train, y_train, X_val, y_val, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Obtain the best learning rate from the dictionary `alpha_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "best_alpha = min(alpha_dict, key=alpha_dict.get)\n",
    "\n",
    "print(best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is yet another sanity check. This function plots the regression lines of out model and the model based on the pseudoinverse calculation. Both models should exhibit the same trend through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ee7d8763464371",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_train[:,1], y_train, 'ro', ms=1, mec='k')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta), 'o')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta_pinv), '-')\n",
    "plt.legend(['Training data', 'Linear regression', 'Best theta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e77c602466fab37d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Multivariate Linear Regression\n",
    "\n",
    "In most cases, you will deal with databases that have more than one feature. It can be as little as two features and up to thousands of features. In those cases, we use a multiple linear regression model. The regression equation is almost the same as the simple linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(\\vec{x}) = \\theta^T \\vec{x} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "\n",
    "If you wrote vectorized code, this part should be straightforward. If your code is not vectorized, you should go back and edit your functions such that they support both multivariate and single variable regression. **Your code should not check the dimensionality of the input before running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15626dda8db26550",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dc0f4dc3491520c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Like in the single variable case, we need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a87b4027bd3bda4b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price', 'id', 'date']).values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa12f54513b1efa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the **same** `preprocess` function you implemented previously. Notice that proper vectorized implementation should work regardless of the dimensionality of the input. You might want to check that your code in the previous parts still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f40a9df530db9399",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3D visualization, we can still observe trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c68216a26a9b5af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = p3.Axes3D(fig)\n",
    "xx = X_train[:, 1][:1000]\n",
    "yy = X_train[:, 2][:1000]\n",
    "zz = y_train[:1000]\n",
    "ax.scatter(xx, yy, zz, marker='o')\n",
    "ax.set_xlabel('bathrooms')\n",
    "ax.set_ylabel('sqft_living')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70fcd47d69caea00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the bias trick again (add a column of ones as the zeroth column in the both the training and validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2985911f4b7af3e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "b1 = np.ones(np.shape(X_train)[0])\n",
    "X_train = np.column_stack((b1, X_train))\n",
    "b1 = np.ones(np.shape(X_val)[0])\n",
    "X_val = np.column_stack((b1, X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b89288ff61c80ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure the functions `compute_cost` , `gradient_descent` , and `pinv`  work on the multi-dimensional dataset. If you make any changes, make sure your code still works on the single variable regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81ab741781b2f6ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]\n",
    "theta = np.ones(shape)\n",
    "J = compute_cost(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f25fb05bd6c648a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shape = X_train.shape[1]\n",
    "theta = np.random.random(shape)\n",
    "iterations = 40000\n",
    "#overflow message due to find_best_alpha() - that is expected.\n",
    "dict_a = find_best_alpha(X_train, y_train, X_val, y_val, iterations)\n",
    "best_alpha = min(dict_a, key=dict_a.get)\n",
    "theta, J_history = efficient_gradient_descent(X_train ,y_train, theta, best_alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-827d1de1293be51f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use visualization to make sure the code works well. Notice we use logarithmic scale for the number of iterations, since gradient descent converges after ~500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fa207b72d2445c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(J_history)), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations - multivariate linear regression')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Find best features for regression\n",
    "\n",
    "Adding additional features to our regression model makes it more complicated but does not necessarily improves performance.\n",
    "Use forward and backward selection and find 4 features that best minimizes the loss. First, we will reload the dataset as a dataframe in order to access the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['price', 'id', 'date']\n",
    "all_features = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "\n",
    "Complete the function `forward_selection`. Train the model using a single feature at a time, and choose the best feature using the validation dataset. Next, check which feature performs best when added to the feature you previously chose. Repeat this process until you reach 4 features + bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def forward_selection(X_train, y_train, X_val, y_val,alpha,num_iter):\n",
    "    \"\"\"\n",
    "    Train the model using the training set using a single feature. \n",
    "    Choose the best feature according to the validation set. Next, \n",
    "    check which feature performs best when added to the feature\n",
    "    you previously chose. Repeat this process until you reach 4 \n",
    "    features and the bias. Don't forget the bias trick.\n",
    "\n",
    "    Returns:\n",
    "    - The names of the best features using forward selection.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    best_features = []\n",
    "\n",
    "    theta_lists = [np.random.random(size=x) for x in range(2,6)]\n",
    "    loss_min = {}\n",
    "    bol = [False if x > 0 else True for x in range(18)]\n",
    "    for i in range(4):\n",
    "        for j in [x for x in range(1, 18) if x not in best_features]:\n",
    "            bol[j] = True\n",
    "            current_X_train = X_train[:, bol]\n",
    "            current_X_val = X_val[:, bol]\n",
    "            current_theta, _ = efficient_gradient_descent(current_X_train, y_train, theta_lists[i], alpha, num_iter)\n",
    "            loss_min[j] = compute_cost(current_X_val, y_val, current_theta)\n",
    "            bol[j] = False\n",
    "        best_fet_iter = min(loss_min, key=loss_min.get)\n",
    "        best_features.append(best_fet_iter)\n",
    "        bol[best_fet_iter] = True\n",
    "        loss_min.clear()\n",
    "    for i in range(4):\n",
    "        best_features[i] = all_features.columns[best_features[i]-1]\n",
    "\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "forward_selection(X_train, y_train, X_val, y_val, 0.1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Feature Selection\n",
    "\n",
    "Train the model with all but one of the features at a time and remove the worst feature (the feature that its absence yields the best loss value using the validation dataset). Next, remove an additional feature along with the feature you previously removed. Repeat this process until you reach 4 features + bias. You are free to use any arguments you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def backward_selection(X_train, y_train, X_val, y_val,alpha,num_iter):\n",
    "    \"\"\"\n",
    "    Train the model using the training set using all but one of the \n",
    "    features at a time. Remove the worst feature according to the \n",
    "    validation set. Next, remove an additional feature along with the \n",
    "    feature you previously removed. Repeat this process until you \n",
    "    reach 4 features and the bias. Don't forget the bias trick.\n",
    "\n",
    "    Returns:\n",
    "    - The names of the best features using backward selection.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    worst_features = []\n",
    "\n",
    "    loss_max = {}\n",
    "    theta_lists = [np.random.random(size=x) for x in range(18)]\n",
    "    bol = [True for x in range(18)]\n",
    "    for i in range(17, 4, -1):\n",
    "        for j in [x for x in range(1, 18) if x not in worst_features]:\n",
    "            bol[j] = False\n",
    "            current_X_train = X_train[:, bol]\n",
    "            current_X_val = X_val[:, bol]\n",
    "            current_theta, _ = efficient_gradient_descent(current_X_train, y_train, theta_lists[i], alpha, num_iter)\n",
    "            loss_max[j] = compute_cost(current_X_val, y_val, current_theta)\n",
    "            bol[j] = True\n",
    "        worst_fet_iter = min(loss_max, key=loss_max.get)\n",
    "        worst_features.append(worst_fet_iter)\n",
    "        bol[worst_fet_iter] = False\n",
    "        loss_max.clear()\n",
    "    best_features = []\n",
    "    for i in range(1,len(bol)):\n",
    "        if bol[i]:\n",
    "            best_features.append(all_features.columns[i-1])\n",
    "\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "backward_selection(X_train, y_train, X_val, y_val, 0.1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rate\n",
    "\n",
    "So far, we kept the learning rate alpha constant during training. However, changing alpha during training might improve convergence in terms of the global minimum found and running time. Implement the adaptive learning rate method based on the gradient descent algorithm above. \n",
    "\n",
    "**Your task is to find proper hyper-parameter values for the adaptive technique and compare this technique to the constant learning rate. Use clear visualizations of the validation loss and the learning rate as a function of the iteration**. \n",
    "\n",
    "Time based decay: this method reduces the learning rate every iteration according to the following formula:\n",
    "\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + D \\cdot t}$$\n",
    "\n",
    "Where $\\alpha_0$ is the original learning rate, $D$ is a decay factor and $t$ is the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "theta = np.ones(X_train.shape[1])\n",
    "def efficient_gradient_descent_Adaptive(X, y, theta, alpha,D, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of your model using the *training set* and stop \n",
    "    the learning process once the improvement of the loss value is smaller \n",
    "    than 1e-8. using Adaptive lerning rate.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - D: the hyper-parameter given to the Adaptive learning rate formula\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "\n",
    "    m = len(X)\n",
    "    current = compute_cost(X, y, theta)\n",
    "    prev = float('inf')\n",
    "    d = D\n",
    "    counter = 1\n",
    "    while ((abs(current - prev) > 1e-8) and (num_iters > 0)):\n",
    "        prev = current\n",
    "        J_history.append(current)\n",
    "        h_theta = np.dot(X, theta)\n",
    "        difference = np.subtract(h_theta, y)\n",
    "        product = np.dot(X.T, difference) * alpha * (1 / m)\n",
    "        theta = np.subtract(theta, product)\n",
    "        current = compute_cost(X, y, theta)\n",
    "        num_iters -= 1\n",
    "        alpha = (alpha)/(1 + d * counter)\n",
    "        counter +=1\n",
    "\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta_adp_0, J_history_adp_0 = efficient_gradient_descent_Adaptive(X_train, y_train, theta, 0.03,0.01, 10000)\n",
    "theta_adp_1, J_history_adp_1 = efficient_gradient_descent_Adaptive(X_train, y_train, theta, 0.03,0.0001, 10000)\n",
    "theta_const_0,J_history_const_0 = efficient_gradient_descent(X_train, y_train, theta, 0.1, 10000)\n",
    "theta_const_1,J_history_const_1 = efficient_gradient_descent(X_train, y_train, theta, 0.000001, 10000)\n",
    "plt.plot(np.arange(len(J_history_adp_0)), J_history_adp_0,label =\"alpha-Adaptive D= 0.3\")\n",
    "plt.plot(np.arange(len(J_history_adp_1)), J_history_adp_1,label =\"alpha-Adaptive D= 0.0001\")\n",
    "plt.plot(np.arange(len(J_history_const_0)), J_history_const_0,label =\"alpha-constant - best\")\n",
    "plt.plot(np.arange(len(J_history_const_1)), J_history_const_1,label =\"alpha-constant - bad\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('validation loss and the learning rate as a function of the iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}